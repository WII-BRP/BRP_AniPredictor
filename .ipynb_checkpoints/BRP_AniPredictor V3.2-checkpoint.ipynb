{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a168f95f-82a3-408d-9984-05215135e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exiftool_path = r\"C:\\Windows\\exiftool.exe\"\n",
    "input_dir = r\"D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\"\n",
    "dest_dir = r\"G:\\Guzzler_data\\Sorted_guzzler_data\\2020\\SudasariACD\\A\\20200321_165610__to__20200505_124728\"\n",
    "Station = \"SudasariACD\"\n",
    "Camera = \"B\"\n",
    "latitude = 26.723574  # Replace with your latitude\n",
    "longitude = 70.634269  # Replace with your longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31da0b8d-53f7-4bbf-98e0-3f2a7f1105c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function for find corrupt files\n",
    "import os\n",
    "\n",
    "def list_corrupt_files_in_directory(directory):\n",
    "    corrupt_files=[]\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                # Use the os.path.getsize() method to check if the file is empty or very small\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                if file_size < 10:  # Adjust the threshold as needed\n",
    "                    corrupt_files.append(file_path)\n",
    "                    print(f\"Corrupt file: {file_path}\")\n",
    "                else:\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                # Handle other exceptions that might occur\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "    return corrupt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c60bbf3a-f3f4-425d-9095-80ef09db4144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupt files : 0 images removed\n"
     ]
    }
   ],
   "source": [
    "#### Find and delete corrupt photos\n",
    "\n",
    "corrupt_files=list_corrupt_files_in_directory(input_dir)\n",
    "if len(corrupt_files) > 0:\n",
    "    for c in corrupt_files:\n",
    "        os.remove(c)\n",
    "print(f\"Corrupt files : {len(corrupt_files)} images removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2664b67-e088-4de1-bc97-1df76be3de7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf5ca0e6-b6e0-4d72-8349-bd9511af1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fucntions for fast renaming and copying\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import exifread\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import math\n",
    "import pyfastcopy\n",
    "import gc\n",
    "\n",
    "def list_files_in_directory(directory):\n",
    "    file_paths = []\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg','.csv','.json')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_paths.append(file_path)\n",
    "            else:\n",
    "                continue\n",
    "    return file_paths\n",
    "\n",
    "def convert_datetime(dt):\n",
    "    try:\n",
    "        dt = pd.to_datetime(dt, format='%Y-%m-%d %H-%M-%S')\n",
    "        return dt.strftime('%Y%m%d_%H%M%S')\n",
    "    except pd.errors.OutOfBoundsDatetime:\n",
    "        return 'OutOfBoundsDatetime'\n",
    "\n",
    "def clean_path(path):\n",
    "    return os.path.normpath(path)\n",
    "\n",
    "def process_image(image_path):\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        tags = exifread.process_file(image_file, details=False)\n",
    "        \n",
    "        directory = os.path.dirname(image_path)\n",
    "        filename = os.path.basename(image_path)\n",
    "        filetype_extension = os.path.splitext(filename)[1]\n",
    "        make = tags.get('Image Make', 'N/A')\n",
    "        model = tags.get('Image Model', 'N/A')\n",
    "        datetime_original = tags.get('EXIF DateTimeOriginal', 'N/A')\n",
    "        \n",
    "        return {\n",
    "            'SourceFile': image_path,\n",
    "            'Directory': directory,\n",
    "            'FileName': filename,\n",
    "            'FileTypeExtension': filetype_extension,\n",
    "            'Make': make,\n",
    "            'Model': model,\n",
    "            'DateTimeOriginal': datetime_original\n",
    "        }\n",
    "\n",
    "def read_exif(image_dir):\n",
    "    file_paths = list_files_in_directory(image_dir)\n",
    "    print(len(file_paths))\n",
    "    with ThreadPoolExecutor(20) as executor:  # Adjust max_workers as needed\n",
    "        image_metadata_list = list(executor.map(process_image, file_paths))\n",
    "    exif_info = pd.DataFrame(image_metadata_list)\n",
    "    return exif_info\n",
    "\n",
    "def create_new_filenames(exif_info):\n",
    "    exif_info = exif_info[exif_info[\"DateTimeOriginal\"] != \"N/A\"]\n",
    "    highest_subfolder_number = get_highest_subfolder_number(dest_dir)\n",
    "    exif_info['Station'] = Station\n",
    "    exif_info['Camera'] = Camera\n",
    "    exif_info['DateTimeOriginal'] = pd.to_datetime(exif_info['DateTimeOriginal'], format='%Y:%m:%d %H:%M:%S')\n",
    "    exif_info['FormattedDateTime'] = exif_info['DateTimeOriginal'].apply(convert_datetime)\n",
    "    exif_info = exif_info.sort_values(by=['Station', 'Camera', 'DateTimeOriginal']).reset_index(drop=True)\n",
    "    exif_info['diff'] = exif_info.groupby(['Station', 'Camera'])['DateTimeOriginal'].diff()\n",
    "    exif_info['image_number']=exif_info.groupby(['Station','Camera']).cumcount()+1\n",
    "    exif_info['Directory'] = exif_info['Directory'].apply(clean_path)\n",
    "    exif_info['SourceFile'] = exif_info['SourceFile'].apply(clean_path)\n",
    "    exif_info['Dest_subfolder_number'] = (highest_subfolder_number + exif_info['image_number'].apply(lambda x: math.ceil(x / 10000))).astype(str)\n",
    "    \n",
    "    subfolders=[]\n",
    "    min_dates=[]\n",
    "    max_dates=[]\n",
    "    for d in exif_info[\"Dest_subfolder_number\"].unique():\n",
    "        subfolders.append(d)\n",
    "        temp_renaming_table = exif_info.loc[exif_info[\"Dest_subfolder_number\"] == d]  \n",
    "        min_date = datetime.datetime.strftime(min(temp_renaming_table.FormattedDateTime.apply(lambda x: datetime.datetime.strptime(x,'%Y%m%d_%H%M%S'))),'%Y%m%d_%H%M%S')\n",
    "        min_dates.append(min_date)\n",
    "        max_date = datetime.datetime.strftime(max(temp_renaming_table.FormattedDateTime.apply(lambda x: datetime.datetime.strptime(x,'%Y%m%d_%H%M%S'))),'%Y%m%d_%H%M%S')\n",
    "        max_dates.append(max_date)\n",
    "    subfolder_intervals_df = pd.DataFrame({\"Dest_subfolder_number\" : subfolders, \"min_date\" : min_dates, \"max_date\" : max_dates})\n",
    "    exif_info = exif_info.merge(subfolder_intervals_df, how = \"left\")\n",
    "    exif_info['Dest_Directory'] = (dest_dir + \"\\\\\" + exif_info['min_date'] + \"__to__\" + exif_info[\"max_date\"]).apply(clean_path)\n",
    "    #exif_info['Dest_Directory'] = (dest_dir + \"\\\\\" + exif_info[\"Dest_subfolder_number\"]).apply(clean_path)\n",
    "\n",
    "    ### Add sequence number\n",
    "    threshold = timedelta(seconds=1)\n",
    "    Sequence = []\n",
    "    for i in range(len(exif_info)):\n",
    "        diff = exif_info['diff'][i]\n",
    "        if pd.isna(diff) or diff > threshold:\n",
    "            sequence = 1\n",
    "        else:\n",
    "            sequence = Sequence[i - 1] + 1\n",
    "        Sequence.append(sequence)\n",
    "    exif_info['Sequence'] = Sequence\n",
    "\n",
    "    ### Construct new filename\n",
    "    exif_info['FileNameNew'] = exif_info['Station'] + '_' + exif_info['Camera'] + '_' + exif_info['FormattedDateTime'] + '(' + exif_info['Sequence'].astype(str) + ')' + exif_info['FileTypeExtension']\n",
    "    exif_info['DestFile'] = (exif_info['Dest_Directory'] + \"\\\\\" + exif_info['FileNameNew']).apply(clean_path)\n",
    "    \n",
    "    return exif_info\n",
    "\n",
    "def get_highest_subfolder_number(dest_dir):\n",
    "    # Get existing subdirectories\n",
    "    if os.path.isdir(dest_dir):\n",
    "        existing_subdirs = [d for d in os.listdir(dest_dir) if os.path.isdir(os.path.join(dest_dir, d))]\n",
    "        subfolder_numbers = [int(dir) for dir in existing_subdirs]\n",
    "        try:\n",
    "            max_number = max(subfolder_numbers)\n",
    "        except:\n",
    "            max_number = 0\n",
    "    else:\n",
    "        max_number = 0\n",
    "    # Return the highest subfolder number or 0 if none exist\n",
    "    return max_number\n",
    "\n",
    "def copy_images_batch(table, batch_size=1000):\n",
    "    src_files=table['SourceFile']\n",
    "    dest_files=table['DestFile']\n",
    "    with ThreadPoolExecutor(20) as exe:\n",
    "        for i in range(0, len(src_files), batch_size):\n",
    "            src_batch = src_files[i:i + batch_size]\n",
    "            dest_batch = dest_files[i:i + batch_size]\n",
    "            \n",
    "            batch_tasks = [exe.submit(shutil.copy, src, dest) for src, dest in zip(src_batch, dest_batch)]\n",
    "            # Wait for all tasks in the batch to complete before proceeding to the next batch\n",
    "            _ = [task.result() for task in batch_tasks]\n",
    "            print(f\"First {i+1 * 1000} images copied at {datetime.datetime.now()}\")\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5922ebe-79df-4a01-8423-474b9439463d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n",
      "Renaming table created in 0:00:00.459744\n",
      "First 1000 images copied at 2024-03-02 17:10:54.876656\n",
      "Renaming and copying completed in 0:00:01.082353\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "###Create Renaming Table\n",
    "exif = read_exif(input_dir)\n",
    "renaming_table=create_new_filenames(exif)\n",
    "print(f\"Renaming table created in {datetime.datetime.now() - start}\")\n",
    "\n",
    "### Copy and rename in batches, based on renaming table\n",
    "unique_directories = set(renaming_table['Dest_Directory'])\n",
    "for d in unique_directories:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "copy_images_batch(renaming_table)\n",
    "print(f\"Renaming and copying completed in {datetime.datetime.now() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "017e3055-9dc0-47c2-a2a9-d75ebd8056a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SourceFile</th>\n",
       "      <th>Directory</th>\n",
       "      <th>FileName</th>\n",
       "      <th>FileTypeExtension</th>\n",
       "      <th>Make</th>\n",
       "      <th>Model</th>\n",
       "      <th>DateTimeOriginal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...</td>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\A</td>\n",
       "      <td>IMG_0001.JPG</td>\n",
       "      <td>.JPG</td>\n",
       "      <td>RECONYX</td>\n",
       "      <td>HC500 HYPERFIRE</td>\n",
       "      <td>2023:05:23 19:35:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...</td>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\A</td>\n",
       "      <td>IMG_0002.JPG</td>\n",
       "      <td>.JPG</td>\n",
       "      <td>RECONYX</td>\n",
       "      <td>HC500 HYPERFIRE</td>\n",
       "      <td>2023:05:23 19:36:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...</td>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\A</td>\n",
       "      <td>IMG_0003.JPG</td>\n",
       "      <td>.JPG</td>\n",
       "      <td>RECONYX</td>\n",
       "      <td>HC500 HYPERFIRE</td>\n",
       "      <td>2023:05:23 19:37:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...</td>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\A</td>\n",
       "      <td>IMG_0004.JPG</td>\n",
       "      <td>.JPG</td>\n",
       "      <td>RECONYX</td>\n",
       "      <td>HC500 HYPERFIRE</td>\n",
       "      <td>2023:05:23 19:38:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...</td>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\A</td>\n",
       "      <td>IMG_0005.JPG</td>\n",
       "      <td>.JPG</td>\n",
       "      <td>RECONYX</td>\n",
       "      <td>HC500 HYPERFIRE</td>\n",
       "      <td>2023:05:23 19:39:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...</td>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\B</td>\n",
       "      <td>IMG_0225.JPG</td>\n",
       "      <td>.JPG</td>\n",
       "      <td>RECONYX</td>\n",
       "      <td>HC500 HYPERFIRE</td>\n",
       "      <td>2023:01:29 11:18:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...</td>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\B</td>\n",
       "      <td>IMG_0226.JPG</td>\n",
       "      <td>.JPG</td>\n",
       "      <td>RECONYX</td>\n",
       "      <td>HC500 HYPERFIRE</td>\n",
       "      <td>2023:01:29 11:18:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...</td>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\B</td>\n",
       "      <td>IMG_0227.JPG</td>\n",
       "      <td>.JPG</td>\n",
       "      <td>RECONYX</td>\n",
       "      <td>HC500 HYPERFIRE</td>\n",
       "      <td>2023:01:29 11:18:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...</td>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\B</td>\n",
       "      <td>IMG_0228.JPG</td>\n",
       "      <td>.JPG</td>\n",
       "      <td>RECONYX</td>\n",
       "      <td>HC500 HYPERFIRE</td>\n",
       "      <td>2023:01:29 11:19:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...</td>\n",
       "      <td>D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\B</td>\n",
       "      <td>IMG_0229.JPG</td>\n",
       "      <td>.JPG</td>\n",
       "      <td>RECONYX</td>\n",
       "      <td>HC500 HYPERFIRE</td>\n",
       "      <td>2023:01:29 11:19:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            SourceFile  \\\n",
       "0    D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...   \n",
       "1    D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...   \n",
       "2    D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...   \n",
       "3    D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...   \n",
       "4    D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...   \n",
       "..                                                 ...   \n",
       "142  D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...   \n",
       "143  D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...   \n",
       "144  D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...   \n",
       "145  D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...   \n",
       "146  D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\...   \n",
       "\n",
       "                                           Directory      FileName  \\\n",
       "0    D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\A  IMG_0001.JPG   \n",
       "1    D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\A  IMG_0002.JPG   \n",
       "2    D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\A  IMG_0003.JPG   \n",
       "3    D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\A  IMG_0004.JPG   \n",
       "4    D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\A  IMG_0005.JPG   \n",
       "..                                               ...           ...   \n",
       "142  D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\B  IMG_0225.JPG   \n",
       "143  D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\B  IMG_0226.JPG   \n",
       "144  D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\B  IMG_0227.JPG   \n",
       "145  D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\B  IMG_0228.JPG   \n",
       "146  D:\\Trial\\Sudasari B\\Sudasari B2\\Test_14022024\\B  IMG_0229.JPG   \n",
       "\n",
       "    FileTypeExtension     Make            Model     DateTimeOriginal  \n",
       "0                .JPG  RECONYX  HC500 HYPERFIRE  2023:05:23 19:35:00  \n",
       "1                .JPG  RECONYX  HC500 HYPERFIRE  2023:05:23 19:36:00  \n",
       "2                .JPG  RECONYX  HC500 HYPERFIRE  2023:05:23 19:37:00  \n",
       "3                .JPG  RECONYX  HC500 HYPERFIRE  2023:05:23 19:38:00  \n",
       "4                .JPG  RECONYX  HC500 HYPERFIRE  2023:05:23 19:39:00  \n",
       "..                ...      ...              ...                  ...  \n",
       "142              .JPG  RECONYX  HC500 HYPERFIRE  2023:01:29 11:18:48  \n",
       "143              .JPG  RECONYX  HC500 HYPERFIRE  2023:01:29 11:18:48  \n",
       "144              .JPG  RECONYX  HC500 HYPERFIRE  2023:01:29 11:18:59  \n",
       "145              .JPG  RECONYX  HC500 HYPERFIRE  2023:01:29 11:19:00  \n",
       "146              .JPG  RECONYX  HC500 HYPERFIRE  2023:01:29 11:19:01  \n",
       "\n",
       "[147 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a8362-e26d-4286-9f79-e88bd35760a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "784d761f-2cd9-43c7-a689-2cbbef8f0c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions for geotagging\n",
    "\n",
    "import subprocess\n",
    "import exiftool\n",
    "import datetime\n",
    "\n",
    "def add_gps_info(image_dir):\n",
    "    command = [exiftool_path, '-GPSLatitude=' + str(latitude), '-GPSLongitude=' + str(longitude), '-overwrite_original','-r',image_dir]\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"GPS information added to {image_dir} successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error while processing {image_dir}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "051c8d66-4039-4a29-b219-6114deb2a2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPS information added to D:\\Trial\\Sudasari B\\Sudasari B2\\Test_Dest_14022024\\20230129_111729__to__20230523_222702 successfully.\n",
      "Geotagging completed in 0:00:03.621962\n"
     ]
    }
   ],
   "source": [
    "### Geotag\n",
    "start = datetime.datetime.now()\n",
    "for d in unique_directories:\n",
    "    add_gps_info(d)\n",
    "end= datetime.datetime.now()\n",
    "print(f\"Geotagging completed in {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0920113-634d-4031-b853-c2d2c6dfe1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce86546d-0618-4f5e-b9f2-96fa02adddc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n",
      "G:\\Guzzler_data\\Sorted_guzzler_data\\2020\\SudasariACD\\A\\20200321_165610__to__20200505_124728\n",
      "G:\\Guzzler_data\\Sorted_guzzler_data\\2020\\SudasariACD\\A\\20200505_124731__to__20200505_155441\n",
      "G:\\Guzzler_data\\Sorted_guzzler_data\\2020\\SudasariACD\\A\\20200505_155442__to__20200508_142416\n",
      "G:\\Guzzler_data\\Sorted_guzzler_data\\2020\\SudasariACD\\A\\20200508_142417__to__20200513_193059\n",
      "G:\\Guzzler_data\\Sorted_guzzler_data\\2020\\SudasariACD\\A\\20200513_193100__to__20200514_105123\n",
      "G:\\Guzzler_data\\Sorted_guzzler_data\\2020\\SudasariACD\\A\\20200514_105123__to__20200514_183349\n",
      "G:\\Guzzler_data\\Sorted_guzzler_data\\2020\\SudasariACD\\A\\20200514_183349__to__20200515_014831\n",
      "G:\\Guzzler_data\\Sorted_guzzler_data\\2020\\SudasariACD\\A\\20200515_014836__to__20200520_171430\n",
      "G:\\Guzzler_data\\Sorted_guzzler_data\\2020\\SudasariACD\\A\\20200520_171431__to__20200607_012708\n",
      "G:\\Guzzler_data\\Sorted_guzzler_data\\2020\\SudasariACD\\A\\20200607_012709__to__20200607_135010\n",
      "G:\\Guzzler_data\\Sorted_guzzler_data\\2020\\SudasariACD\\A\\20200607_135010__to__20200607_183555\n",
      "G:\\Guzzler_data\\Sorted_guzzler_data\\2020\\SudasariACD\\A\\20200607_183559__to__20200608_082113\n",
      "G:\\Guzzler_data\\Sorted_guzzler_data\\2020\\SudasariACD\\A\\20200608_082332__to__20200608_144007\n",
      "Loading Models...\n",
      "0:01:01\n",
      "\n",
      "G:\\Guzzler_data\\Sorted_guzzler_data\\2020\\SudasariACD\\A\\20200321_165610__to__20200505_124728\n",
      "15759\n",
      "Megadetector model\n",
      "Megadetector output file already exists.. Going for species classification\n",
      "Saving detections at G:\\Guzzler_data\\Sorted_guzzler_data\\2020\\SudasariACD\\A\\20200321_165610__to__20200505_124728\\SudasariACD_A_20200321_165610__to__20200505_124728_megadetector.json...\n",
      "Generating detections.csv...\n",
      "Images already cropped...\n",
      "Order level classification already complete\n",
      "Predicting Small Carnivores...\n",
      "1/1 [==============================] - 8s 8s/step\n",
      "Predicting Ungulates...\n",
      "5/5 [==============================] - 5s 253ms/step\n",
      "Moving Ungulates and Small Carnivores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying images: 0it [00:01, ?it/s]\n",
      "Removing source images: 0it [00:01, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 0 images copied and removed at 2024-03-04 22:28:11.285874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Logs\n",
      "\n",
      "G:\\Guzzler_data\\Sorted_guzzler_data\\2020\\SudasariACD\\A\\20200505_124731__to__20200505_155441\n",
      "20415\n",
      "Megadetector model\n",
      "Detecting objects in 20415 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████████████▊                                                           | 27/100 [09:01<24:17, 19.97s/it]"
     ]
    }
   ],
   "source": [
    "### Run BRP_Anipredictor\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from functions import *\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "from Megadetector import *\n",
    "\n",
    "try:\n",
    "    print(f\"Models loaded : {areModelsLoaded}\")\n",
    "except:\n",
    "    areModelsLoaded = False\n",
    "log = {}\n",
    "now = datetime.now()\n",
    "\n",
    "inp = \"YES\"\n",
    "\n",
    "## CHECK FOR GPU\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "cpu = tf.config.experimental.list_physical_devices(\"CPU\")\n",
    "\n",
    "if gpus:\n",
    "    print(\"GPU available\")\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"No GPUs available.\")\n",
    "    \n",
    "## GET SUBDIRECTORIES\n",
    "\n",
    "sub_dirs = []\n",
    "for dirpath, dirnames, filenames in os.walk(dest_dir):\n",
    "    if not \"Cropped_images\" in dirpath and not dest_dir == dirpath: # 'not \"Cropped_images\" in dirnames and' at the start if required\n",
    "        if dirpath.split(\"\\\\\")[-1] not in [\"Cropped_images\",\"GIB\", \"Goat_Sheep\", \"Hare\", \"Human\", \"Raptor\", \"Small Bird\", \"Small Carnivore\", \"Ungulate\", \"Vehicle\", \"Wild Pig\"]:\n",
    "            sub_dirs.append(dirpath)\n",
    "        \n",
    "if sub_dirs == []:\n",
    "    sub_dirs.append(dest_dir)\n",
    "\n",
    "for i in sub_dirs:\n",
    "    print(i)\n",
    "\n",
    "if inp == \"YES\":\n",
    "    ## LOAD MODELS\n",
    "    print(\"Loading Models...\")\n",
    "    order_level_class_names = [\"GIB\", \"Goat_Sheep\", \"Hare\", \"Human\", \"Raptor\", \"Small Bird\", \"Small Carnivore\", \"Ungulate\", \"Vehicle\", \"Wild Pig\"]\n",
    "    order_level_class_names.sort()\n",
    "    ungulate_class_names = [\"Camel\", \"Chinkara\", \"Nilgai\", \"Cattle\"]\n",
    "    ungulate_class_names.sort() \n",
    "    small_carnivores_class_names = [\"Dog\", \"Desert Cat\", \"Fox\"]\n",
    "    small_carnivores_class_names.sort()\n",
    "    \n",
    "    model_load_start=time.time()\n",
    "    if areModelsLoaded == False:\n",
    "        order_level_model_path = os.path.join(os.getcwd(), r\"Models\\Refined_Hierarchical.ckpt\")\n",
    "        order_level_model = tf.keras.models.load_model(order_level_model_path)\n",
    "        \n",
    "        ungulate_model_path = os.path.join(os.getcwd(), r\"Models\\Efficient_Net_Ungulates_3.ckpt\")\n",
    "        ungulate_model = tf.keras.models.load_model(ungulate_model_path)   \n",
    "        \n",
    "        small_carnivore_model_path = os.path.join(os.getcwd(), r\"Models\\Efficient_Net_Small_Carnivores_1.ckpt\")\n",
    "        small_carnivore_model= tf.keras.models.load_model(small_carnivore_model_path)\n",
    "        \n",
    "        areModelsLoaded = True\n",
    "        \n",
    "    model_load_end = time.time()\n",
    "    model_load_time = str(timedelta(seconds=round(model_load_end - model_load_start)))\n",
    "    log.update({\"Species Model Load Time\" : model_load_time})\n",
    "    print(model_load_time)\n",
    "    \n",
    "    for data_dir in sub_dirs:\n",
    "        print()\n",
    "        print(data_dir)\n",
    "        ## CREATE LOGS\n",
    "        \n",
    "        log.update({\"Run timestamp\" : str(now)})\n",
    "        # log.update({\"GPU\" : gpus})\n",
    "        log.update({\"GPU Available for Classification : \" : gpu_name})\n",
    "        # log.update({\"CPU\" : cpu})\n",
    "        num_images = 0\n",
    "        files = os.listdir(data_dir)\n",
    "        for f in files:\n",
    "            if f.lower().endswith(\".jpg\"):\n",
    "                num_images += 1\n",
    "        log.update({\"Num images\" : num_images})\n",
    "        print(num_images)\n",
    "        \n",
    "        ## RUN MEGADETECTOR AND CREATE DETECTIONS.DF\n",
    "        \n",
    "        megadetector_start = time.time()\n",
    "        json_dir, megadetector_log = megadetector(data_dir, num_images)\n",
    "        if not megadetector_log == {}:\n",
    "            log.update(megadetector_log)\n",
    "        else:\n",
    "            megadetector_end = time.time()\n",
    "            megadetector_time = str(timedelta(seconds=round(megadetector_end - megadetector_start)))\n",
    "            log.update({\"Megadetector time\" : megadetector_time})\n",
    "            log.update({\"Megadetector Filename\" : os.path.basename(json_dir)})\n",
    "\n",
    "        df_detections = get_detection_df(data_dir, json_dir)\n",
    "        \n",
    "        ## CROP IMAGES\n",
    "        \n",
    "        cropping_start = time.time() \n",
    "        cropped_images = os.path.join(data_dir,r\"Cropped_images\\*\")\n",
    "        cropped_dir = clean_path(\"\\\\\".join(cropped_images.split(\"\\\\\")[:-1]))\n",
    "        if not os.path.exists(cropped_dir):\n",
    "            print(\"Cropping Images\")\n",
    "            df_crop = df_detections.copy()\n",
    "            df_crop[\"Cropped_image_directory\"] = cropped_dir\n",
    "            df_crop[\"Cropped_image_name\"] = df_crop[\"Filename\"] + \"_\" + df_crop[\"Detection_number\"].astype(str) + \".jpg\"\n",
    "            df_crop[\"Cropped_image_path\"] = (cropped_dir + \"\\\\\" + df_crop[\"Cropped_image_name\"]).apply(clean_path)\n",
    "            try:\n",
    "                df_crop=crop_images_batch_gpu(df_crop,512)\n",
    "            except:\n",
    "                df_crop = crop_images_batch(df_crop,512)\n",
    "                print(f\"Cropping exception occured\")\n",
    "        else:\n",
    "            print(\"Images already cropped...\")\n",
    "        cropping_end = time.time()\n",
    "        cropping_time = str(timedelta(seconds=round(cropping_end - cropping_start)))\n",
    "        log.update({\"Cropping Time\" : cropping_time})\n",
    "        log.update({\"Number of Detections\" : len(df_detections)})\n",
    "        \n",
    "        ## ORDER LEVEL PREDICTIONS\n",
    "        \n",
    "        if os.path.exists(os.path.join(cropped_dir,r\"Ungulate\")) or os.path.exists(os.path.join(cropped_dir,r\"Small Carnivore\")):\n",
    "            print(\"Order level classification already complete\")\n",
    "        else:\n",
    "            order_level_start = time.time()\n",
    "            print(\"Predicting Order Level Classes...\")\n",
    "            df_temp, num_cropped = predict_lower_level_species(data_dir, \n",
    "                                                               r\"Cropped_images\\*\", \n",
    "                                                               order_level_class_names,\n",
    "                                                               order_level_model,\n",
    "                                                               level = \"Order\")\n",
    "            \n",
    "            order_level_end = time.time()\n",
    "            df_order = pd.merge(df_crop, df_temp, on='Cropped_image_name', how='left')\n",
    "            df_order[\"Order_pred\"] = df_order[\"Order_pred\"].fillna(\"Error\")\n",
    "            df_order[\"Order_dir\"] = (cropped_dir + \"\\\\\" + df_order[\"Order_pred\"]).apply(clean_path)\n",
    "            df_order[\"Order_level_path\"] = (df_order[\"Order_dir\"] + \"\\\\\" + df_order[\"Cropped_image_name\"]).apply(clean_path)\n",
    "            \n",
    "            unique_directories = set(df_order['Order_dir'])\n",
    "            for directory in unique_directories:\n",
    "                os.makedirs(directory, exist_ok=True)\n",
    "            \n",
    "            print(\"Moving Order Level Images...\")\n",
    "            move_images_batch(df_order[\"Cropped_image_path\"], df_order[\"Order_level_path\"])\n",
    "            \n",
    "            order_shift_end = time.time()\n",
    "            order_pred_time = str(timedelta(seconds=round(order_level_end - order_level_start)))\n",
    "            order_shift_time = str(timedelta(seconds=round(order_shift_end - order_level_end)))\n",
    "            log.update({\"Order Level Prediction Time\" : order_shift_time})\n",
    "            log.update({\"Order Level Shifting Time\" : order_shift_time})\n",
    "        \n",
    "        ## SMALL CARNIVORES PREDICT\n",
    "        small_carnivores_start = time.time()\n",
    "        if os.path.exists(os.path.join(cropped_dir,r\"Small Carnivore\")):\n",
    "            print(\"Predicting Small Carnivores...\")\n",
    "            df_small_carnivore, num_small_carnivores = predict_lower_level_species(cropped_dir, \n",
    "                                                                                   r\"Small Carnivore\\*\", \n",
    "                                                                                   small_carnivores_class_names,\n",
    "                                                                                   small_carnivore_model,\n",
    "                                                                                   level = \"Species\")\n",
    "            df_small_carnivore[\"Order_pred\"] = r\"Small Carnivore\"\n",
    "            df_small_carnivore[\"Order_dir\"] = (cropped_dir + \"\\\\\" + df_small_carnivore[\"Order_pred\"]).apply(clean_path)\n",
    "            df_small_carnivore[\"Order_level_path\"] = (df_small_carnivore[\"Order_dir\"] + \"\\\\\" + df_small_carnivore[\"Cropped_image_name\"]).apply(clean_path)\n",
    "            small_carnivores_end = time.time()\n",
    "            small_carnivore_time = str(timedelta(seconds=round(small_carnivores_end - small_carnivores_start)))\n",
    "            log.update({\"Number of Small Carnivores Images\" : num_small_carnivores})\n",
    "            log.update({\"Small Carnivore Model Pred Time\" : small_carnivore_time})\n",
    "        else:\n",
    "            df_small_carnivore = pd.DataFrame(columns=['Cropped_image_name','Species_pred','Species_pred_prob'])\n",
    "       \n",
    "        ## UNGULATES PREDICT\n",
    "\n",
    "        if os.path.exists(os.path.join(cropped_dir,r\"Ungulate\")) and len(os.listdir(os.path.join(cropped_dir,r\"Ungulate\"))) > 0:\n",
    "            ungulate_start = time.time()\n",
    "            print(\"Predicting Ungulates...\")\n",
    "            df_ungulate, num_ungulates = predict_lower_level_species(cropped_dir, \n",
    "                                                                     r\"Ungulate\\*\", \n",
    "                                                                     ungulate_class_names,\n",
    "                                                                     ungulate_model,\n",
    "                                                                     level = \"Species\")\n",
    "            df_ungulate[\"Order_pred\"] = r\"Ungulate\"\n",
    "            df_ungulate[\"Order_dir\"] = (cropped_dir + \"\\\\\" + df_ungulate[\"Order_pred\"]).apply(clean_path)\n",
    "            df_ungulate[\"Order_level_path\"] = (df_ungulate[\"Order_dir\"] + \"\\\\\" + df_ungulate[\"Cropped_image_name\"]).apply(clean_path)\n",
    "            ungulate_end = time.time()\n",
    "            ungulate_time = str(timedelta(seconds=round(ungulate_end - ungulate_start)))\n",
    "            log.update({\"Number of Ungulates Images\" : num_ungulates})\n",
    "            log.update({\"Ungulate Model Pred Time\" : ungulate_time})\n",
    "        else:\n",
    "            df_ungulate = pd.DataFrame(columns=['Cropped_image_name','Species_pred','Species_pred_prob'])\n",
    "        \n",
    "        species_shift_start = time.time()\n",
    "        df_species = pd.concat([df_small_carnivore,df_ungulate])\n",
    "        df_species[\"Species_dir\"] = (cropped_dir + \"\\\\\" + df_species[\"Species_pred\"]).apply(clean_path)\n",
    "        df_species[\"Species_level_path\"] = (df_species[\"Species_dir\"] + \"\\\\\" + df_species[\"Cropped_image_name\"]).apply(clean_path)\n",
    "        \n",
    "        #df_move = pd.merge(df_species, df_order, on='Cropped_image_name', how='left')\n",
    "        df_move = df_species[df_species[\"Order_level_path\"] != df_species[\"Species_level_path\"]]\n",
    "        unique_directories = set(df_move['Species_dir'])\n",
    "        for directory in unique_directories:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        print(\"Moving Ungulates and Small Carnivores...\")\n",
    "        #copy_images_batch(df_move[\"Order_level_path\"], df_move[\"Species_level_path\"])\n",
    "        #delete_images_batch(df_move[\"Order_level_path\"])\n",
    "        move_images_batch(df_move[\"Order_level_path\"], df_move[\"Species_level_path\"])\n",
    "        \n",
    "        species_shift_end = time.time()\n",
    "        species_shift_time = str(timedelta(seconds=round(species_shift_end - species_shift_start)))\n",
    "        species_level_time = str(timedelta(seconds=round(species_shift_end - small_carnivores_start)))\n",
    "        log.update({\"Species Level Shift Imgs Time\" : species_shift_time})\n",
    "        log.update({\"Species Level Predict and Shift\" : species_level_time})\n",
    "        \n",
    "        ## SAVE FINAL PREDICTIONS.CSV\n",
    "        if os.path.exists(os.path.join(data_dir,r\"predictions.csv\")):\n",
    "            predictions = pd.read_csv(os.path.join(data_dir,r\"predictions.csv\"))\n",
    "            preds = predictions.copy().dropna()\n",
    "            preds_all = pd.concat([preds,df_species[~df_species[\"Cropped_image_name\"].isin(preds[\"Cropped_image_name\"].tolist())]])\n",
    "            preds_all = preds_all[[\"Cropped_image_name\",\"Species_pred\",\"Species_pred_prob\",\"Species_dir\",\"Species_level_path\"]]\n",
    "            predictions.drop(columns = [\"Species_pred\",\"Species_pred_prob\",\"Species_dir\",\"Species_level_path\"], inplace = True)\n",
    "            df_final = pd.merge(predictions, preds_all, how='left', on = \"Cropped_image_name\")\n",
    "        else:\n",
    "            df_final = pd.merge(df_order, df_species.drop(columns=['Order_dir', 'Order_level_path']), on='Cropped_image_name', how='left')\n",
    "            df_final.drop(columns=['Order_dir', 'Order_level_path','Cropped_image_path'], inplace=True)\n",
    "        df_final_path = os.path.join(data_dir, \"predictions.csv\")\n",
    "        df_final.to_csv(df_final_path, index=False)\n",
    "        \n",
    "        ## SAVE LOG FILE\n",
    "        print(\"Saving Logs\")\n",
    "        log_file_name = \"_\".join(data_dir.split(\"\\\\\")[-3:])\n",
    "        log_file_path = os.path.join(data_dir, f\"{log_file_name}_log.json\")\n",
    "        with open(log_file_path, \"w\") as f:\n",
    "            json.dump(log, f, indent=2)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9bb62c-490e-4076-b40e-7c7e00c503e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = r\"G:\\Guzzler_data\\Sorted_guzzler_data\\2020\\SudasariACD\\A\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
