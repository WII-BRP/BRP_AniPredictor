{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e368899c-3f7f-412f-bb6a-f746e7224bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "exiftool_path = r\"C:\\Windows\\exiftool.exe\"\n",
    "input_dir = r\"F:\\Pred_Estimation_2023\\PPC\\331_117_4_1622\"\n",
    "dest_dir = r\"F:\\Pred_Estimation_2023\\Timelapse\\PPC\\331_117_4\"  ### Motion = 100CUDDY and Timelapse = 200CUDDY\n",
    "Station = \"PPC\"\n",
    "Camera = \"331_117_4\"\n",
    "latitude = 26.65711  # Replace with your latitude\n",
    "longitude = 70.60218  # Replace with your longitude\n",
    "folder = \"200CUDDY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24ba5622-fcde-433f-9536-cce3a8e79fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function for find corrupt files\n",
    "import os\n",
    "\n",
    "def list_corrupt_files_in_directory(directory):\n",
    "    corrupt_files=[]\n",
    "    file_list=[]\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_list.append(file_path)\n",
    "            try:\n",
    "                # Use the os.path.getsize() method to check if the file is empty or very small\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                if file_size < 10:  # Adjust the threshold as needed\n",
    "                    corrupt_files.append(file_path)\n",
    "                    print(f\"Corrupt file: {file_path}\")\n",
    "                else:\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                # Handle other exceptions that might occur\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "    print(f\"Total files checked: {len(file_list)}, Corrupt files: {len(corrupt_files)}\")\n",
    "    return corrupt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89d33695-71d8-4b35-8063-810cb616390c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files checked: 30696, Corrupt files: 0\n",
      "Corrupt files : 0 images removed\n"
     ]
    }
   ],
   "source": [
    "#### Find and delete corrupt photos\n",
    "\n",
    "corrupt_files=list_corrupt_files_in_directory(input_dir)\n",
    "if len(corrupt_files) > 0:\n",
    "    for c in corrupt_files:\n",
    "        os.remove(c)\n",
    "print(f\"Corrupt files : {len(corrupt_files)} images removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afe4bb61-58c9-41d2-8e55-a678e2000426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import exifread\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import shutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import math\n",
    "import pyfastcopy\n",
    "import gc\n",
    "\n",
    "def list_files_in_directory(directory):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_paths.append(file_path)\n",
    "            else:\n",
    "                continue\n",
    "    return file_paths\n",
    "\n",
    "def convert_datetime(dt):\n",
    "    try:\n",
    "        dt = pd.to_datetime(dt, format='%Y-%m-%d %H-%M-%S')\n",
    "        return dt.strftime('%Y%m%d_%H%M%S')\n",
    "    except pd.errors.OutOfBoundsDatetime:\n",
    "        return 'OutOfBoundsDatetime'\n",
    "\n",
    "def clean_path(path):\n",
    "    return os.path.normpath(path)\n",
    "\n",
    "def process_image(image_path):\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        tags = exifread.process_file(image_file, details=False)\n",
    "        \n",
    "        directory = os.path.dirname(image_path)\n",
    "        filename = os.path.basename(image_path)\n",
    "        filetype_extension = os.path.splitext(filename)[1]\n",
    "        make = tags.get('Image Make', 'N/A')\n",
    "        model = tags.get('Image Model', 'N/A')\n",
    "        datetime_original = tags.get('EXIF DateTimeOriginal', 'N/A')\n",
    "        \n",
    "        return {\n",
    "            'SourceFile': image_path,\n",
    "            'Directory': directory,\n",
    "            'FileName': filename,\n",
    "            'FileTypeExtension': filetype_extension,\n",
    "            'Make': make,\n",
    "            'Model': model,\n",
    "            'DateTimeOriginal': datetime_original\n",
    "        }\n",
    "\n",
    "def read_exif(image_dir):\n",
    "    file_paths=[]\n",
    "    files = list_files_in_directory(input_dir)\n",
    "    for f in files:\n",
    "        if f.split(\"\\\\\")[5] == folder and not f.split(\"\\\\\")[-3] == \"Cropped images\":\n",
    "            file_paths.append(f)\n",
    "    print(len(file_paths))\n",
    "    with ThreadPoolExecutor(20) as executor:  # Adjust max_workers as needed\n",
    "        image_metadata_list = list(executor.map(process_image, file_paths))\n",
    "    exif_info = pd.DataFrame(image_metadata_list)\n",
    "    return exif_info\n",
    "\n",
    "def create_new_filenames(exif_info):\n",
    "    exif_info = exif_info[exif_info[\"DateTimeOriginal\"] != \"N/A\"]\n",
    "    highest_subfolder_number = get_highest_subfolder_number(dest_dir)\n",
    "    exif_info['Station'] = Station\n",
    "    exif_info['Camera'] = Camera\n",
    "    exif_info['DateTimeOriginal'] = pd.to_datetime(exif_info['DateTimeOriginal'], format='%Y:%m:%d %H:%M:%S')\n",
    "    exif_info['FormattedDateTime'] = exif_info['DateTimeOriginal'].apply(convert_datetime)\n",
    "    exif_info = exif_info.sort_values(by=['Station', 'Camera', 'DateTimeOriginal']).reset_index(drop=True)\n",
    "    exif_info['diff'] = exif_info.groupby(['Station', 'Camera'])['DateTimeOriginal'].diff()\n",
    "    exif_info['image_number']=exif_info.groupby(['Station','Camera']).cumcount()+1\n",
    "    exif_info['Directory'] = exif_info['Directory'].apply(clean_path)\n",
    "    exif_info['SourceFile'] = exif_info['SourceFile'].apply(clean_path)\n",
    "    exif_info['Dest_subfolder_number'] = (highest_subfolder_number + exif_info['image_number'].apply(lambda x: math.ceil(x / 10000))).astype(str)\n",
    "    \n",
    "    subfolders=[]\n",
    "    min_dates=[]\n",
    "    max_dates=[]\n",
    "    for d in exif_info[\"Dest_subfolder_number\"].unique():\n",
    "        subfolders.append(d)\n",
    "        temp_renaming_table = exif_info.loc[exif_info[\"Dest_subfolder_number\"] == d]  \n",
    "        min_date = datetime.datetime.strftime(min(temp_renaming_table.FormattedDateTime.apply(lambda x: datetime.datetime.strptime(x,'%Y%m%d_%H%M%S'))),'%Y%m%d_%H%M%S')\n",
    "        min_dates.append(min_date)\n",
    "        max_date = datetime.datetime.strftime(max(temp_renaming_table.FormattedDateTime.apply(lambda x: datetime.datetime.strptime(x,'%Y%m%d_%H%M%S'))),'%Y%m%d_%H%M%S')\n",
    "        max_dates.append(max_date)\n",
    "    subfolder_intervals_df = pd.DataFrame({\"Dest_subfolder_number\" : subfolders, \"min_date\" : min_dates, \"max_date\" : max_dates})\n",
    "    exif_info = exif_info.merge(subfolder_intervals_df, how = \"left\")\n",
    "    exif_info['Dest_Directory'] = (dest_dir + \"\\\\\" + exif_info['min_date'] + \"__to__\" + exif_info[\"max_date\"]).apply(clean_path)\n",
    "    #exif_info['Dest_Directory'] = (dest_dir + \"\\\\\" + exif_info[\"Dest_subfolder_number\"]).apply(clean_path)\n",
    "\n",
    "    ### Add sequence number\n",
    "    threshold = timedelta(seconds=1)\n",
    "    Sequence = []\n",
    "    for i in range(len(exif_info)):\n",
    "        diff = exif_info['diff'][i]\n",
    "        if pd.isna(diff) or diff > threshold:\n",
    "            sequence = 1\n",
    "        else:\n",
    "            sequence = Sequence[i - 1] + 1\n",
    "        Sequence.append(sequence)\n",
    "    exif_info['Sequence'] = Sequence\n",
    "\n",
    "    ### Construct new filename\n",
    "    exif_info['FileNameNew'] = exif_info['Station'] + '_' + exif_info['Camera'] + '_' + exif_info['FormattedDateTime'] + '(' + exif_info['Sequence'].astype(str) + ')' + exif_info['FileTypeExtension']\n",
    "    exif_info['DestFile'] = (exif_info['Dest_Directory'] + \"\\\\\" + exif_info['FileNameNew']).apply(clean_path)\n",
    "    \n",
    "    return exif_info\n",
    "\n",
    "def get_highest_subfolder_number(dest_dir):\n",
    "    # Get existing subdirectories\n",
    "    if os.path.isdir(dest_dir):\n",
    "        existing_subdirs = [d for d in os.listdir(dest_dir) if os.path.isdir(os.path.join(dest_dir, d))]\n",
    "        subfolder_numbers = len(existing_subdirs)\n",
    "        try:\n",
    "            max_number = max(subfolder_numbers)-1\n",
    "        except:\n",
    "            max_number = 0\n",
    "    else:\n",
    "        max_number = 0\n",
    "    # Return the highest subfolder number or 0 if none exist\n",
    "    return max_number\n",
    "\n",
    "def copy_images_batch(table, batch_size=1000):\n",
    "    src_files=table['SourceFile']\n",
    "    dest_files=table['DestFile']\n",
    "    with ThreadPoolExecutor(20) as exe:\n",
    "        for i in range(0, len(src_files), batch_size):\n",
    "            src_batch = src_files[i:i + batch_size]\n",
    "            dest_batch = dest_files[i:i + batch_size]\n",
    "            \n",
    "            batch_tasks = [exe.submit(shutil.copy, src, dest) for src, dest in zip(src_batch, dest_batch)]\n",
    "            # Wait for all tasks in the batch to complete before proceeding to the next batch\n",
    "            _ = [task.result() for task in batch_tasks]\n",
    "            print(f\"First {i+1 * 1000} images copied at {datetime.datetime.now()}\")\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "817a799d-4e57-473b-ae10-6744ed1a6860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_7224\\4116072902.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exif_info['Station'] = Station\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_7224\\4116072902.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exif_info['Camera'] = Camera\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_7224\\4116072902.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exif_info['DateTimeOriginal'] = pd.to_datetime(exif_info['DateTimeOriginal'], format='%Y:%m:%d %H:%M:%S')\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_7224\\4116072902.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exif_info['FormattedDateTime'] = exif_info['DateTimeOriginal'].apply(convert_datetime)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming table created in 0:01:26.737173\n",
      "First 1000 images copied at 2024-03-06 19:47:45.128537\n",
      "First 2000 images copied at 2024-03-06 19:47:58.049888\n",
      "First 3000 images copied at 2024-03-06 19:48:18.230038\n",
      "First 4000 images copied at 2024-03-06 19:48:45.213409\n",
      "First 5000 images copied at 2024-03-06 19:49:18.871426\n",
      "First 6000 images copied at 2024-03-06 19:49:58.485976\n",
      "First 7000 images copied at 2024-03-06 19:50:46.365407\n",
      "First 8000 images copied at 2024-03-06 19:51:39.901765\n",
      "First 9000 images copied at 2024-03-06 19:52:39.534223\n",
      "First 10000 images copied at 2024-03-06 19:53:45.117258\n",
      "First 11000 images copied at 2024-03-06 19:53:51.744379\n",
      "First 12000 images copied at 2024-03-06 19:54:03.303556\n",
      "First 13000 images copied at 2024-03-06 19:54:23.145946\n",
      "First 14000 images copied at 2024-03-06 19:54:49.127138\n",
      "First 15000 images copied at 2024-03-06 19:55:22.740535\n",
      "First 16000 images copied at 2024-03-06 19:56:02.698827\n",
      "First 17000 images copied at 2024-03-06 19:56:49.297455\n",
      "First 18000 images copied at 2024-03-06 19:57:42.166256\n",
      "First 19000 images copied at 2024-03-06 19:58:40.946677\n",
      "First 20000 images copied at 2024-03-06 19:59:45.505007\n",
      "First 21000 images copied at 2024-03-06 19:59:53.793813\n",
      "First 22000 images copied at 2024-03-06 20:00:05.554246\n",
      "First 23000 images copied at 2024-03-06 20:00:24.406967\n",
      "First 24000 images copied at 2024-03-06 20:00:50.430603\n",
      "First 25000 images copied at 2024-03-06 20:01:23.122625\n",
      "First 26000 images copied at 2024-03-06 20:02:02.660413\n",
      "First 27000 images copied at 2024-03-06 20:02:09.296335\n",
      "Renaming and copying completed in 0:15:58.580232\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "###Create Renaming Table\n",
    "exif = read_exif(input_dir)\n",
    "renaming_table=create_new_filenames(exif)\n",
    "print(f\"Renaming table created in {datetime.datetime.now() - start}\")\n",
    "\n",
    "### Copy and rename in batches, based on renaming table\n",
    "unique_directories = set(renaming_table['Dest_Directory'])\n",
    "for d in unique_directories:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "copy_images_batch(renaming_table)\n",
    "print(f\"Renaming and copying completed in {datetime.datetime.now() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "527789b9-02ce-4361-8796-e21fb7aaa0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions for geotagging\n",
    "\n",
    "import subprocess\n",
    "import exiftool\n",
    "import datetime\n",
    "\n",
    "def add_gps_info(image_dir):\n",
    "    command = [exiftool_path, '-GPSLatitude=' + str(latitude), '-GPSLongitude=' + str(longitude), '-overwrite_original','-r',image_dir]\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "        print(f\"GPS information added to {image_dir} successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error while processing {image_dir}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "374daa50-41e4-47a7-a692-052a7856620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPS information added to F:\\Pred_Estimation_2023\\Timelapse\\PPC\\331_117_4\\20240116_122934__to__20240117_053024 successfully.\n",
      "GPS information added to F:\\Pred_Estimation_2023\\Timelapse\\PPC\\331_117_4\\20240113_092242__to__20240114_131032 successfully.\n",
      "GPS information added to F:\\Pred_Estimation_2023\\Timelapse\\PPC\\331_117_4\\20240114_131042__to__20240116_122924 successfully.\n",
      "Geotagging completed in 0:30:16.477599\n"
     ]
    }
   ],
   "source": [
    "### Geotag\n",
    "start = datetime.datetime.now()\n",
    "for d in unique_directories:\n",
    "    add_gps_info(d)\n",
    "end= datetime.datetime.now()\n",
    "print(f\"Geotagging completed in {end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72ee7b03-94dc-40e9-8e52-4d67058d3165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded : True\n",
      "GPU available\n",
      "F:\\Pred_Estimation_2023\\Timelapse\\PPC\\331_117_4\\20240116_122934__to__20240117_053024\n",
      "F:\\Pred_Estimation_2023\\Timelapse\\PPC\\331_117_4\\20240113_092242__to__20240114_131032\n",
      "F:\\Pred_Estimation_2023\\Timelapse\\PPC\\331_117_4\\20240114_131042__to__20240116_122924\n",
      "Loading Models...\n",
      "0:00:00\n",
      "\n",
      "F:\\Pred_Estimation_2023\\Timelapse\\PPC\\331_117_4\\20240116_122934__to__20240117_053024\n",
      "6126\n",
      "Megadetector model\n",
      "Detecting objects in 6126 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [18:46<00:00, 11.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding Boxes Created\n",
      "Generating detections.csv...\n",
      "Cropping Images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Order Level Classes...\n",
      "1/1 [==============================] - 1s 938ms/step\n",
      "Moving Order Level Images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying images: 100%|██████████████████████████████████████████████████████████████████| 22/22 [00:01<00:00, 17.31it/s]\n",
      "Removing source images: 100%|██████████████████████████████████████████████████████████| 22/22 [00:33<00:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 22 images copied and removed at 2024-03-06 20:51:50.643442\n",
      "Predicting Small Carnivores...\n",
      "1/1 [==============================] - 0s 60ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Ungulates...\n",
      "1/1 [==============================] - 1s 696ms/step\n",
      "Moving Ungulates and Small Carnivores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying images: 100%|██████████████████████████████████████████████████████████████████| 17/17 [00:01<00:00, 13.83it/s]\n",
      "Removing source images: 100%|██████████████████████████████████████████████████████████| 17/17 [00:33<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 17 images copied and removed at 2024-03-06 20:52:26.552353\n",
      "Saving Logs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F:\\Pred_Estimation_2023\\Timelapse\\PPC\\331_117_4\\20240113_092242__to__20240114_131032\n",
      "10000\n",
      "Megadetector model\n",
      "Detecting objects in 10000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|██████████████████████████████████████████████████████████████████████████▌      | 92/100 [27:32<02:23, 17.96s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 99\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m## RUN MEGADETECTOR AND CREATE DETECTIONS.DF\u001b[39;00m\n\u001b[0;32m     98\u001b[0m megadetector_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 99\u001b[0m json_dir, megadetector_log \u001b[38;5;241m=\u001b[39m \u001b[43mmegadetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m megadetector_log \u001b[38;5;241m==\u001b[39m {}:\n\u001b[0;32m    101\u001b[0m     log\u001b[38;5;241m.\u001b[39mupdate(megadetector_log)\n",
      "File \u001b[1;32mD:\\WII_BRP\\In_situ\\Camera_trapping\\BRP_AniPredictor\\Megadetector.py:33\u001b[0m, in \u001b[0;36mmegadetector\u001b[1;34m(img_dir, num_images)\u001b[0m\n\u001b[0;32m     29\u001b[0m prev_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(command,\n\u001b[0;32m     31\u001b[0m         stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mSTDOUT, bufsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     32\u001b[0m         universal_newlines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m p\u001b[38;5;241m.\u001b[39mstdout:\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model in\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cameratraps-env\\lib\\encodings\\cp1252.py:22\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIncrementalDecoder\u001b[39;00m(codecs\u001b[38;5;241m.\u001b[39mIncrementalDecoder):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Run BRP_Anipredictor\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from functions import *\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "from Megadetector import *\n",
    "\n",
    "try:\n",
    "    print(f\"Models loaded : {areModelsLoaded}\")\n",
    "except:\n",
    "    areModelsLoaded = False\n",
    "log = {}\n",
    "now = datetime.now()\n",
    "\n",
    "inp = \"YES\"\n",
    "\n",
    "## CHECK FOR GPU\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "cpu = tf.config.experimental.list_physical_devices(\"CPU\")\n",
    "\n",
    "if gpus:\n",
    "    print(\"GPU available\")\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"No GPUs available.\")\n",
    "    \n",
    "## GET SUBDIRECTORIES\n",
    "\n",
    "sub_dirs = []\n",
    "for dirpath, dirnames, filenames in os.walk(dest_dir):\n",
    "    if not \"Cropped_images\" in dirpath and not dest_dir == dirpath: # 'not \"Cropped_images\" in dirnames and' at the start if required\n",
    "        if dirpath.split(\"\\\\\")[-1] not in [\"Cropped_images\",\"GIB\", \"Goat_Sheep\", \"Hare\", \"Human\", \"Raptor\", \"Small Bird\", \"Small Carnivore\", \"Ungulate\", \"Vehicle\", \"Wild Pig\"]:\n",
    "            sub_dirs.append(dirpath)\n",
    "        \n",
    "if sub_dirs == []:\n",
    "    sub_dirs.append(dest_dir)\n",
    "\n",
    "for i in sub_dirs:\n",
    "    print(i)\n",
    "\n",
    "if inp == \"YES\":\n",
    "    ## LOAD MODELS\n",
    "    print(\"Loading Models...\")\n",
    "    order_level_class_names = [\"GIB\", \"Goat_Sheep\", \"Hare\", \"Human\", \"Raptor\", \"Small Bird\", \"Small Carnivore\", \"Ungulate\", \"Vehicle\", \"Wild Pig\"]\n",
    "    order_level_class_names.sort()\n",
    "    ungulate_class_names = [\"Camel\", \"Chinkara\", \"Nilgai\", \"Cattle\"]\n",
    "    ungulate_class_names.sort() \n",
    "    small_carnivores_class_names = [\"Dog\", \"Desert Cat\", \"Fox\"]\n",
    "    small_carnivores_class_names.sort()\n",
    "    \n",
    "    model_load_start=time.time()\n",
    "    if areModelsLoaded == False:\n",
    "        order_level_model_path = os.path.join(os.getcwd(), r\"Models\\Refined_Hierarchical.ckpt\")\n",
    "        order_level_model = tf.keras.models.load_model(order_level_model_path)\n",
    "        \n",
    "        ungulate_model_path = os.path.join(os.getcwd(), r\"Models\\Efficient_Net_Ungulates_3.ckpt\")\n",
    "        ungulate_model = tf.keras.models.load_model(ungulate_model_path)   \n",
    "        \n",
    "        small_carnivore_model_path = os.path.join(os.getcwd(), r\"Models\\Efficient_Net_Small_Carnivores_1.ckpt\")\n",
    "        small_carnivore_model= tf.keras.models.load_model(small_carnivore_model_path)\n",
    "        \n",
    "        areModelsLoaded = True\n",
    "        \n",
    "    model_load_end = time.time()\n",
    "    model_load_time = str(timedelta(seconds=round(model_load_end - model_load_start)))\n",
    "    log.update({\"Species Model Load Time\" : model_load_time})\n",
    "    print(model_load_time)\n",
    "    \n",
    "    for data_dir in sub_dirs:\n",
    "        print()\n",
    "        print(data_dir)\n",
    "        ## CREATE LOGS\n",
    "        \n",
    "        log.update({\"Run timestamp\" : str(now)})\n",
    "        # log.update({\"GPU\" : gpus})\n",
    "        log.update({\"GPU Available for Classification : \" : gpu_name})\n",
    "        # log.update({\"CPU\" : cpu})\n",
    "        num_images = 0\n",
    "        files = os.listdir(data_dir)\n",
    "        for f in files:\n",
    "            if f.lower().endswith(\".jpg\"):\n",
    "                num_images += 1\n",
    "        log.update({\"Num images\" : num_images})\n",
    "        print(num_images)\n",
    "        \n",
    "        ## RUN MEGADETECTOR AND CREATE DETECTIONS.DF\n",
    "        \n",
    "        megadetector_start = time.time()\n",
    "        json_dir, megadetector_log = megadetector(data_dir, num_images)\n",
    "        if not megadetector_log == {}:\n",
    "            log.update(megadetector_log)\n",
    "        else:\n",
    "            megadetector_end = time.time()\n",
    "            megadetector_time = str(timedelta(seconds=round(megadetector_end - megadetector_start)))\n",
    "            log.update({\"Megadetector time\" : megadetector_time})\n",
    "            log.update({\"Megadetector Filename\" : os.path.basename(json_dir)})\n",
    "\n",
    "        df_detections = get_detection_df(data_dir, json_dir)\n",
    "        \n",
    "        ## CROP IMAGES\n",
    "        \n",
    "        cropping_start = time.time() \n",
    "        cropped_images = os.path.join(data_dir,r\"Cropped_images\\*\")\n",
    "        cropped_dir = clean_path(\"\\\\\".join(cropped_images.split(\"\\\\\")[:-1]))\n",
    "        if not os.path.exists(cropped_dir):\n",
    "            print(\"Cropping Images\")\n",
    "            df_crop = df_detections.copy()\n",
    "            df_crop[\"Cropped_image_directory\"] = cropped_dir\n",
    "            df_crop[\"Cropped_image_name\"] = df_crop[\"Filename\"] + \"_\" + df_crop[\"Detection_number\"].astype(str) + \".jpg\"\n",
    "            df_crop[\"Cropped_image_path\"] = (cropped_dir + \"\\\\\" + df_crop[\"Cropped_image_name\"]).apply(clean_path)\n",
    "            try:\n",
    "                df_crop=crop_images_batch_gpu(df_crop,512)\n",
    "            except:\n",
    "                df_crop = crop_images_batch(df_crop,512)\n",
    "                print(f\"Cropping exception occured\")\n",
    "        else:\n",
    "            df_crop = df_detections.copy()\n",
    "            df_crop[\"Cropped_image_directory\"] = cropped_dir\n",
    "            df_crop[\"Cropped_image_name\"] = df_crop[\"Filename\"] + \"_\" + df_crop[\"Detection_number\"].astype(str) + \".jpg\"\n",
    "            df_crop[\"Cropped_image_path\"] = (cropped_dir + \"\\\\\" + df_crop[\"Cropped_image_name\"]).apply(clean_path)\n",
    "            print(\"Images already cropped...\")\n",
    "        cropping_end = time.time()\n",
    "        cropping_time = str(timedelta(seconds=round(cropping_end - cropping_start)))\n",
    "        log.update({\"Cropping Time\" : cropping_time})\n",
    "        log.update({\"Number of Detections\" : len(df_detections)})\n",
    "        \n",
    "        ## ORDER LEVEL PREDICTIONS\n",
    "        \n",
    "        if os.path.exists(os.path.join(cropped_dir,r\"Ungulate\")) or os.path.exists(os.path.join(cropped_dir,r\"Small Carnivore\")) or os.path.exists(os.path.join(cropped_dir,r\"Others\")):\n",
    "            print(\"Order level classification already complete\")\n",
    "        else:\n",
    "            order_level_start = time.time()\n",
    "            print(\"Predicting Order Level Classes...\")\n",
    "            df_temp, num_cropped = predict_lower_level_species(data_dir, \n",
    "                                                               r\"Cropped_images\\*\", \n",
    "                                                               order_level_class_names,\n",
    "                                                               order_level_model,\n",
    "                                                               level = \"Order\")\n",
    "            \n",
    "            order_level_end = time.time()\n",
    "            df_order = pd.merge(df_crop, df_temp, on='Cropped_image_name', how='left')\n",
    "            df_order[\"Order_pred\"] = df_order[\"Order_pred\"].fillna(\"Error\")\n",
    "            df_order[\"Order_dir\"] = (cropped_dir + \"\\\\\" + df_order[\"Order_pred\"]).apply(clean_path)\n",
    "            df_order[\"Order_level_path\"] = (df_order[\"Order_dir\"] + \"\\\\\" + df_order[\"Cropped_image_name\"]).apply(clean_path)\n",
    "            \n",
    "            unique_directories = set(df_order['Order_dir'])\n",
    "            for directory in unique_directories:\n",
    "                os.makedirs(directory, exist_ok=True)\n",
    "            \n",
    "            print(\"Moving Order Level Images...\")\n",
    "            move_images_batch(df_order[\"Cropped_image_path\"], df_order[\"Order_level_path\"])\n",
    "            \n",
    "            order_shift_end = time.time()\n",
    "            order_pred_time = str(timedelta(seconds=round(order_level_end - order_level_start)))\n",
    "            order_shift_time = str(timedelta(seconds=round(order_shift_end - order_level_end)))\n",
    "            log.update({\"Order Level Prediction Time\" : order_shift_time})\n",
    "            log.update({\"Order Level Shifting Time\" : order_shift_time})\n",
    "        \n",
    "        ## SMALL CARNIVORES PREDICT\n",
    "        small_carnivores_start = time.time()\n",
    "        if os.path.exists(os.path.join(cropped_dir,r\"Small Carnivore\")) and len(os.listdir(os.path.join(cropped_dir,r\"Small Carnivore\"))) > 0:\n",
    "            print(\"Predicting Small Carnivores...\")\n",
    "            df_small_carnivore, num_small_carnivores = predict_lower_level_species(cropped_dir, \n",
    "                                                                                   r\"Small Carnivore\\*\", \n",
    "                                                                                   small_carnivores_class_names,\n",
    "                                                                                   small_carnivore_model,\n",
    "                                                                                   level = \"Species\")\n",
    "            df_small_carnivore[\"Order_pred\"] = r\"Small Carnivore\"\n",
    "            df_small_carnivore[\"Order_dir\"] = (cropped_dir + \"\\\\\" + df_small_carnivore[\"Order_pred\"]).apply(clean_path)\n",
    "            df_small_carnivore[\"Order_level_path\"] = (df_small_carnivore[\"Order_dir\"] + \"\\\\\" + df_small_carnivore[\"Cropped_image_name\"]).apply(clean_path)\n",
    "            small_carnivores_end = time.time()\n",
    "            small_carnivore_time = str(timedelta(seconds=round(small_carnivores_end - small_carnivores_start)))\n",
    "            log.update({\"Number of Small Carnivores Images\" : num_small_carnivores})\n",
    "            log.update({\"Small Carnivore Model Pred Time\" : small_carnivore_time})\n",
    "        else:\n",
    "            df_small_carnivore = pd.DataFrame(columns=['Cropped_image_name','Species_pred','Species_pred_prob'])\n",
    "       \n",
    "        ## UNGULATES PREDICT\n",
    "\n",
    "        if os.path.exists(os.path.join(cropped_dir,r\"Ungulate\")) and len(os.listdir(os.path.join(cropped_dir,r\"Ungulate\"))) > 0:\n",
    "            ungulate_start = time.time()\n",
    "            print(\"Predicting Ungulates...\")\n",
    "            df_ungulate, num_ungulates = predict_lower_level_species(cropped_dir, \n",
    "                                                                     r\"Ungulate\\*\", \n",
    "                                                                     ungulate_class_names,\n",
    "                                                                     ungulate_model,\n",
    "                                                                     level = \"Species\")\n",
    "            df_ungulate[\"Order_pred\"] = r\"Ungulate\"\n",
    "            df_ungulate[\"Order_dir\"] = (cropped_dir + \"\\\\\" + df_ungulate[\"Order_pred\"]).apply(clean_path)\n",
    "            df_ungulate[\"Order_level_path\"] = (df_ungulate[\"Order_dir\"] + \"\\\\\" + df_ungulate[\"Cropped_image_name\"]).apply(clean_path)\n",
    "            ungulate_end = time.time()\n",
    "            ungulate_time = str(timedelta(seconds=round(ungulate_end - ungulate_start)))\n",
    "            log.update({\"Number of Ungulates Images\" : num_ungulates})\n",
    "            log.update({\"Ungulate Model Pred Time\" : ungulate_time})\n",
    "        else:\n",
    "            df_ungulate = pd.DataFrame(columns=['Cropped_image_name','Species_pred','Species_pred_prob'])\n",
    "        \n",
    "        species_shift_start = time.time()\n",
    "        df_species = pd.concat([df_small_carnivore,df_ungulate])\n",
    "        df_species[\"Species_dir\"] = (cropped_dir + \"\\\\\" + df_species[\"Species_pred\"]).apply(clean_path)\n",
    "        df_species[\"Species_level_path\"] = (df_species[\"Species_dir\"] + \"\\\\\" + df_species[\"Cropped_image_name\"]).apply(clean_path)\n",
    "        \n",
    "        #df_move = pd.merge(df_species, df_order, on='Cropped_image_name', how='left')\n",
    "        try:\n",
    "            df_move = df_species[df_species[\"Order_level_path\"] != df_species[\"Species_level_path\"]]\n",
    "            unique_directories = set(df_move['Species_dir'])\n",
    "            for directory in unique_directories:\n",
    "                os.makedirs(directory, exist_ok=True)\n",
    "            print(\"Moving Ungulates and Small Carnivores...\")\n",
    "            #copy_images_batch(df_move[\"Order_level_path\"], df_move[\"Species_level_path\"])\n",
    "            #delete_images_batch(df_move[\"Order_level_path\"])\n",
    "            move_images_batch(df_move[\"Order_level_path\"], df_move[\"Species_level_path\"])\n",
    "        except:\n",
    "            print(\"No files to move\")\n",
    "            \n",
    "        species_shift_end = time.time()\n",
    "        species_shift_time = str(timedelta(seconds=round(species_shift_end - species_shift_start)))\n",
    "        species_level_time = str(timedelta(seconds=round(species_shift_end - small_carnivores_start)))\n",
    "        log.update({\"Species Level Shift Imgs Time\" : species_shift_time})\n",
    "        log.update({\"Species Level Predict and Shift\" : species_level_time})\n",
    "        \n",
    "        ## SAVE FINAL PREDICTIONS.CSV\n",
    "        if os.path.exists(os.path.join(data_dir,r\"predictions.csv\")):\n",
    "            predictions = pd.read_csv(os.path.join(data_dir,r\"predictions.csv\"))\n",
    "            preds = predictions.copy().dropna()\n",
    "            preds_all = pd.concat([preds,df_species[~df_species[\"Cropped_image_name\"].isin(preds[\"Cropped_image_name\"].tolist())]])\n",
    "            preds_all = preds_all[[\"Cropped_image_name\",\"Species_pred\",\"Species_pred_prob\",\"Species_dir\",\"Species_level_path\"]]\n",
    "            predictions.drop(columns = [\"Species_pred\",\"Species_pred_prob\",\"Species_dir\",\"Species_level_path\"], inplace = True)\n",
    "            df_final = pd.merge(predictions, preds_all, how='left', on = \"Cropped_image_name\")\n",
    "        else:\n",
    "            try:\n",
    "                df_final = pd.merge(df_order, df_species.drop(columns=['Order_pred','Order_dir', 'Order_level_path']), on='Cropped_image_name', how='left')\n",
    "            except:\n",
    "                df_final = pd.merge(df_order, df_species, on='Cropped_image_name', how='left')\n",
    "            df_final.drop(columns=['Order_dir', 'Order_level_path','Cropped_image_path'], inplace=True)\n",
    "        df_final_path = os.path.join(data_dir, \"predictions.csv\")\n",
    "        df_final.to_csv(df_final_path, index=False)\n",
    "        \n",
    "        ## SAVE LOG FILE\n",
    "        print(\"Saving Logs\")\n",
    "        log_file_name = \"_\".join(data_dir.split(\"\\\\\")[-3:])\n",
    "        log_file_path = os.path.join(data_dir, f\"{log_file_name}_log.json\")\n",
    "        with open(log_file_path, \"w\") as f:\n",
    "            json.dump(log, f, indent=2)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f3b26-019c-40c8-b669-a87c8ba2f069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cameratraps-env] *",
   "language": "python",
   "name": "conda-env-cameratraps-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
