{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626f4753-d851-4dbe-b1bb-a1e57ea2bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Give data directory and run all the rows\n",
    "\n",
    "data_dir=r\"E:\\Camera_Trapping\\Guzzler_data\\2023\\CameraTrap\\RKVY\\GIB\\2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0049ae16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\Camera_Trapping\\\\Guzzler_data\\\\2023\\\\CameraTrap\\\\RKVY\\\\GIB\\\\2']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "sub_directories=[os.path.join(data_dir, d) for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "if not sub_directories:\n",
    "    sub_directories= [data_dir]\n",
    "sub_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d449c9-61ec-4630-9785-691467efc5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"No GPUs available.\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from subprocess import Popen, PIPE \n",
    "import re\n",
    "import subprocess\n",
    "import concurrent.futures\n",
    "import threading\n",
    "IMG_SIZE = (224,224)\n",
    "\n",
    "def check_existing_file(img_dir):\n",
    "    output_file_path = os.path.join(img_dir, \"output.json\")\n",
    "    if os.path.exists(output_file_path):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def megadetector(img_dir, num_images):\n",
    "    print(\"Megadetector model\")\n",
    "\n",
    "    local_detector_path = os.path.join(os.getcwd(), \"cameratraps\", \"detection\", \"run_detector_batch.py\")\n",
    "    megadetector_path = os.path.join(os.getcwd(), \"md_v5a.0.0.pt\")\n",
    "    json_dir = os.path.join(img_dir, \"output.json\")\n",
    "\n",
    "    if check_existing_file(img_dir) == 1:\n",
    "        # self.output_label.configure(text = f\"Megadetector output file already exists.. Going for species classification\")\n",
    "        print(\"Megadetector output file already exists.. Going for species classification\")\n",
    "        return json_dir\n",
    "    \n",
    "    print(local_detector_path, megadetector_path, json_dir)\n",
    "\n",
    "    command = [sys.executable,\n",
    "                local_detector_path,\n",
    "                megadetector_path,\n",
    "                img_dir,\n",
    "                json_dir,\n",
    "                \"--recursive\"]\n",
    "\n",
    "    with Popen(command,\n",
    "            stdout=subprocess.PIPE, stderr=subprocess.STDOUT, bufsize=1, shell=True,\n",
    "            universal_newlines=True) as p:\n",
    "        for line in p.stdout:\n",
    "            if line.startswith(\"Loaded model in\"):\n",
    "                print(line)\n",
    "            \n",
    "            elif \"%\" in line[0:4]:\n",
    "                percentage = re.search(\"\\d*%\", line[0:4])[0][:-1]\n",
    "                \n",
    "            print(line)\n",
    "    \n",
    "    print(\"Bounding Boxes Created\")\n",
    "\n",
    "    return json_dir\n",
    "\n",
    "def get_detection_df(img_path, json_dir):\n",
    "    print(\"Generating detections.csv...\")\n",
    "\n",
    "    with open(json_dir, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        df = pd.DataFrame(data[\"images\"])\n",
    "\n",
    "    records = []\n",
    "    for i, row in df.iterrows():\n",
    "        filepath = row[\"file\"]\n",
    "        filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "        detections = row[\"detections\"]\n",
    "        for j, detection in enumerate(detections):\n",
    "            area = detection[\"bbox\"][2] * detection[\"bbox\"][3]\n",
    "            y_position = detection[\"bbox\"][1]\n",
    "            if (\n",
    "                detection[\"conf\"] > 0.1\n",
    "                and area > 0.001\n",
    "                and not (area <= 0.01 and y_position > 0.6)\n",
    "            ):\n",
    "                if detection[\"category\"] == '1':\n",
    "                    category = \"Animal\"\n",
    "                elif detection[\"category\"] == '2':\n",
    "                    category = \"Person\"\n",
    "                else:\n",
    "                    category = \"Vehicle\"\n",
    "\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"Filepath\": filepath,\n",
    "                        \"Filename\": filename,\n",
    "                        \"Detection_number\": j + 1,\n",
    "                        \"Category\": category,\n",
    "                        \"Detection_Confidence\": detection[\"conf\"],\n",
    "                        \"Detection_bbox\": detection[\"bbox\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    new_df = pd.DataFrame(records)\n",
    "    new_df[\"File_directory\"] = new_df[\"Filepath\"].apply(os.path.dirname)\n",
    "    df_path = os.path.join(img_path, \"detections.csv\")\n",
    "    new_df.to_csv(df_path, index=False)\n",
    "    \n",
    "    small_obj_df = new_df[new_df[\"Category\"] == \"Small Object\"]\n",
    "    if not small_obj_df.empty:\n",
    "        small_obj_df_path = os.path.join(img_path, \"small_objects.csv\")\n",
    "        small_obj_df.to_csv(small_obj_df_path, index=False)\n",
    "\n",
    "    return new_df\n",
    "    \n",
    "def crop_img(img_dir, bbox):\n",
    "    img = Image.open(img_dir)\n",
    "    x,y,w,h = tuple(i for i in bbox)\n",
    "    mul_x = img.size[0]\n",
    "    mul_y = img.size[1]\n",
    "    w = w * mul_x\n",
    "    h = h * mul_y\n",
    "    x1 = x * mul_x\n",
    "    x2 = x * mul_x + w\n",
    "    y1 = y * mul_y\n",
    "    y2 = y * mul_y + h\n",
    "    cropped = img.crop((x1,y1,x2, y2))\n",
    "    return cropped\n",
    "\n",
    "def run_order_level_model(pred_img, order_level_model):\n",
    "    pred = order_level_model.predict(pred_img)\n",
    "    pred = np.squeeze(pred)\n",
    "    order_level_pred_prob = round(np.max(pred,axis=-1), 2)\n",
    "    if order_level_pred_prob >= 0.8:\n",
    "        pred_class = order_level_class_names[np.argmax(pred,axis=-1)]\n",
    "    else:\n",
    "        pred_class = \"Others\" \n",
    "    return pred_class, order_level_pred_prob\n",
    "\n",
    "def run_species_level_model(pred_img, order_pred, ungulate_model, small_carnivore_model):\n",
    "    if order_pred == \"Ungulate\":\n",
    "        species_model = ungulate_model\n",
    "        species_class_names=ungulate_class_names\n",
    "    else:\n",
    "        species_model = small_carnivore_model\n",
    "        species_class_names=small_carnivores_class_names\n",
    "    pred = species_model.predict(pred_img)\n",
    "    pred = np.squeeze(pred)\n",
    "    species_pred_prob= round(np.max(pred,axis=-1),2)\n",
    "    if species_pred_prob >= 0.8:\n",
    "        pred_class = species_class_names[np.argmax(pred,axis=-1)]\n",
    "    else:\n",
    "        pred_class = order_pred\n",
    "    return pred_class,species_pred_prob\n",
    "\n",
    "def process_images(df,models):\n",
    "    order_preds = []\n",
    "    order_pred_probs = []\n",
    "    species_preds=[]\n",
    "    species_pred_probs=[]\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        filepath = row[\"Filepath\"]\n",
    "        filename = row[\"Filename\"]\n",
    "        detection = row[\"Detection_number\"]\n",
    "        bbox = row[\"Detection_bbox\"]\n",
    "        category = row[\"Category\"]\n",
    "        directory = row[\"File_directory\"]\n",
    "        \n",
    "        cropped_img = crop_img(filepath, bbox)\n",
    "        pred_img = np.array(cropped_img)\n",
    "        pred_img = tf.image.resize(pred_img, size= IMG_SIZE,method = \"area\")\n",
    "        pred_img = tf.expand_dims(pred_img, axis=0)\n",
    "        \n",
    "        order_pred = category\n",
    "        order_pred_prob = np.nan\n",
    "        species_pred = category\n",
    "        species_pred_prob = np.nan\n",
    "\n",
    "        if category not in [\"Vehicle\", \"Person\"]:\n",
    "            order_pred, order_pred_prob = run_order_level_model(pred_img, models[0])\n",
    "            if order_pred in [\"Ungulate\", \"Small Carnivore\"]:\n",
    "                species_pred, species_pred_prob = run_species_level_model(pred_img, order_pred, models[1], models[2])\n",
    "            else:\n",
    "                species_pred = order_pred\n",
    "            \n",
    "        order_preds.append(order_pred)\n",
    "        order_pred_probs.append(order_pred_prob)\n",
    "        species_preds.append(species_pred)\n",
    "        species_pred_probs.append(species_pred_prob)\n",
    "        \n",
    "        cropped_dir = os.path.join(directory,f\"Cropped_images\\\\{species_pred}\")\n",
    "        os.makedirs(cropped_dir, exist_ok=True)\n",
    "        cropped_img.save(os.path.join(cropped_dir,f\"{filename}_{detection}.JPG\"))\n",
    "        \n",
    "    df[\"Order_pred\"] = order_preds\n",
    "    df[\"Order_pred_prob\"] = order_pred_probs\n",
    "    df[\"Species_pred\"] = species_preds\n",
    "    df[\"Species_pred_prob\"] = species_pred_probs\n",
    "\n",
    "    df[\"Cropped_image_dir\"] = df[\"File_directory\"] + \"\\\\Cropped_images\\\\\" + df[\"Species_pred\"]\n",
    "    df[\"Detection_number\"] = df[\"Detection_number\"].astype(str)\n",
    "    df[\"Cropped_image_name\"] = df[\"Filename\"] + \"_\" + df[\"Detection_number\"]\n",
    "    df[\"Cropped_image_path\"] = df[\"Cropped_image_dir\"] + \"\\\\\" + df[\"Cropped_image_name\"] + \".JPG\"\n",
    "    df=df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Define a lock for thread safety when modifying shared data\n",
    "lock = threading.Lock()\n",
    "\n",
    "# Define the function for processing a single image\n",
    "def process_one_image(row, models):\n",
    "    filepath = row[0]\n",
    "    filename = row[1]\n",
    "    detection = row[2]\n",
    "    bbox = row[5]\n",
    "    category = row[3]\n",
    "    directory = row[6]\n",
    "    \n",
    "    order_level_model, ungulate_model, small_carnivore_model = models\n",
    "    cropped_img = crop_img(filepath, bbox)\n",
    "    pred_img = np.array(cropped_img)\n",
    "    pred_img = tf.image.resize(pred_img, size=IMG_SIZE, method=\"area\")\n",
    "    pred_img = np.expand_dims(pred_img, axis=0)\n",
    "\n",
    "    order_pred = category\n",
    "    order_pred_prob = np.nan\n",
    "    species_pred = category\n",
    "    species_pred_prob = np.nan\n",
    "\n",
    "    if category not in [\"Vehicle\", \"Person\"]:\n",
    "        order_pred, order_pred_prob = run_order_level_model(pred_img, models[0])\n",
    "        if order_pred in [\"Ungulate\", \"Small Carnivore\"]:\n",
    "            species_pred, species_pred_prob = run_species_level_model(pred_img, order_pred, models[1], models[2])\n",
    "        else:\n",
    "            species_pred = order_pred\n",
    "\n",
    "    cropped_dir = os.path.join(directory, f\"Cropped_images\\\\{species_pred}\")\n",
    "    os.makedirs(cropped_dir, exist_ok=True)\n",
    "    cropped_img.save(os.path.join(cropped_dir, f\"{filename}_{detection}.JPG\"))\n",
    "\n",
    "    # Ensure thread safety when modifying shared data\n",
    "    with lock:\n",
    "        return order_pred, order_pred_prob, species_pred, species_pred_prob\n",
    "\n",
    "def process_images_batch(df, batch_size, models):\n",
    "    order_preds = []\n",
    "    order_pred_probs = []\n",
    "    species_preds = []\n",
    "    species_pred_probs = []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Split the DataFrame into batches\n",
    "    num_batches, remainder = divmod(len(df), batch_size)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        batch = df[i * batch_size: (i + 1) * batch_size]\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(process_one_image, row, models) for row in batch.itertuples(index=False, name=None)]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "\n",
    "    if remainder > 0:\n",
    "        remaining_data = df[num_batches * batch_size:]\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(process_one_image, row, models) for row in remaining_data.itertuples(index=False, name=None)]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        \n",
    "        #executor = ThreadPoolExecutor(max_workers=4)  # Adjust max_workers as needed\n",
    "        # List of inputs (assuming 'remaining_data' is an iterable)\n",
    "        #list_of_inputs = [(row, models) for row in remaining_data.itertuples(index=False, name=None)]\n",
    "        # Use executor.map to apply the function to inputs in parallel\n",
    "        #results = list(executor.map(process_one_image, list_of_inputs))\n",
    "        # Close the executor when done\n",
    "        #executor.shutdown()\n",
    "\n",
    "    for result in results:\n",
    "        order_pred, order_pred_prob, species_pred, species_pred_prob = result\n",
    "        order_preds.append(order_pred)\n",
    "        order_pred_probs.append(order_pred_prob)\n",
    "        species_preds.append(species_pred)\n",
    "        species_pred_probs.append(species_pred_prob)\n",
    "\n",
    "    df[\"Order_pred\"] = order_preds\n",
    "    df[\"Order_pred_prob\"] = order_pred_probs\n",
    "    df[\"Species_pred\"] = species_preds\n",
    "    df[\"Species_pred_prob\"] = species_pred_probs\n",
    "\n",
    "    df[\"Cropped_image_dir\"] = df[\"File_directory\"] + \"\\\\Cropped_images\\\\\" + df[\"Species_pred\"]\n",
    "    df[\"Detection_number\"] = df[\"Detection_number\"].astype(str)\n",
    "    df[\"Cropped_image_name\"] = df[\"Filename\"] + \"_\" + df[\"Detection_number\"]\n",
    "    df[\"Cropped_image_path\"] = df[\"Cropped_image_dir\"] + \"\\\\\" + df[\"Cropped_image_name\"] + \".JPG\"\n",
    "    return df\n",
    "\n",
    "log = {}\n",
    "now = datetime.now()\n",
    "log.update({\"Run timestamp\" : str(now)})\n",
    "num_images = 0\n",
    "for _,_,files in os.walk(data_dir):\n",
    "    num_images += len(files) \n",
    "log.update({\"Num images\" : num_images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c9e6842-fd06-4113-817b-726340985046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:59\n"
     ]
    }
   ],
   "source": [
    "#### Load models\n",
    "\n",
    "models = []\n",
    "\n",
    "model_load_start=time.time()\n",
    "order_level_model_path = os.path.join(os.getcwd(), r\"Models\\Refined_Hierarchical.ckpt\")\n",
    "order_level_model = tf.keras.models.load_model(order_level_model_path)\n",
    "models.append(order_level_model)\n",
    "order_level_class_names = [\"GIB\", \"Goat_Sheep\", \"Hare\", \"Human\", \"Raptor\", \"Small Bird\", \"Small Carnivore\", \"Ungulate\", \"Vehicle\", \"Wild Pig\"]\n",
    "order_level_class_names.sort()\n",
    "\n",
    "ungulate_model_path = os.path.join(os.getcwd(), r\"Models\\Efficient_Net_Ungulates_3.ckpt\")\n",
    "ungulate_model = tf.keras.models.load_model(ungulate_model_path)\n",
    "models.append(ungulate_model)\n",
    "ungulate_class_names = [\"Camel\", \"Chinkara\", \"Nilgai\", \"Cattle\"]\n",
    "ungulate_class_names.sort()     \n",
    "\n",
    "small_carnivore_model_path = os.path.join(os.getcwd(), r\"Models\\Efficient_Net_Small_Carnivores_1.ckpt\")\n",
    "small_carnivore_model= tf.keras.models.load_model(small_carnivore_model_path)\n",
    "models.append(small_carnivore_model)\n",
    "small_carnivores_class_names = [\"Dog\", \"Desert Cat\", \"Fox\"]\n",
    "small_carnivores_class_names.sort()\n",
    "\n",
    "model_load_end = time.time()\n",
    "model_load_time = str(timedelta(seconds=round(model_load_end - model_load_start)))\n",
    "log.update({\"Model_load_time\" : model_load_time})\n",
    "print(model_load_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac05bd5-b275-4244-88ab-add2cc6e9494",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Common command for running all models\n",
    "\n",
    "def detect_and_classify(data_dir, models):\n",
    "    ##### Code for running megadetector\n",
    "    megadetector_start = time.time()\n",
    "    json_dir = megadetector(data_dir, num_images)\n",
    "    df_detections = get_detection_df(data_dir, json_dir)\n",
    "    megadetector_end = time.time()\n",
    "    megadetector_time = str(timedelta(seconds=round(megadetector_end - megadetector_start)))\n",
    "    log.update({\"Megadetector_time\" : megadetector_time})\n",
    "    \n",
    "    #### Code for animal classification on top of detections\n",
    "    image_classification_start = time.time()\n",
    "    df_final = process_images(df_detections, models)\n",
    "    image_classification_end = time.time()\n",
    "    image_classification_time = str(timedelta(seconds=round(image_classification_end - image_classification_start)))\n",
    "    log.update({\"Image_classification_time\" : image_classification_time})\n",
    "    df_path = os.path.join(data_dir, \"predictions.csv\")\n",
    "    df_final.to_csv(df_path, index=False)    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af1462d-0001-413d-9a5a-353b3a0366ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for d in sub_directories:\n",
    "    df_final = detect_and_classify(d, models)\n",
    "end = time.time()\n",
    "duration = str(timedelta(seconds=round(end - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c81aaad-22f2-4bb0-93fd-df239c575583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80906\n"
     ]
    }
   ],
   "source": [
    "print(len(df_final))#duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f4ef70f-719e-4253-ac2e-0afa595e02fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.745648148148148"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc803d3-2b98-44e8-a262-db1ada6ba816",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Code for running megadetector\n",
    "\n",
    "megadetector_start = time.time()\n",
    "json_dir = megadetector(data_dir, num_images)\n",
    "df_detections = get_detection_df(data_dir, json_dir)\n",
    "megadetector_end = time.time()\n",
    "megadetector_time = str(timedelta(seconds=round(megadetector_end - megadetector_start)))\n",
    "log.update({\"Megadetector_time\" : megadetector_time})\n",
    "print(megadetector_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54184b4-0ef7-41bb-a287-8e8b631959e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Code for animal classification on top of detections\n",
    "\n",
    "image_classification_start = time.time()\n",
    "df_final = process_images_batch(df_detections,32)\n",
    "image_classification_end = time.time()\n",
    "image_classification_time = str(timedelta(seconds=round(image_classification_end - image_classification_start)))\n",
    "log.update({\"Image_classification_time\" : image_classification_time})\n",
    "print(image_classification_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcc23eed-988b-4505-b1ec-1419c14a584a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:59\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'megadetector_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_load_time)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmegadetector_time\u001b[49m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_classification_time)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df_final))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'megadetector_time' is not defined"
     ]
    }
   ],
   "source": [
    "print(model_load_time)\n",
    "print(megadetector_time)\n",
    "print(image_classification_time)\n",
    "print(len(df_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17906bb2-9b82-44fc-8e3a-c89417bd9d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2['area'] = df_final['Detection bbox'].apply(lambda x: x[2] * x[3])\n",
    "df_final2['y_position'] = df_final['Detection bbox'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd09573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc855a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab4b28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b49d84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
